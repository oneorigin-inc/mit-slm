services:
  ollama:
    build:
      context: ..
      dockerfile: docker/Dockerfile.ollama
    image: docker-ollama
    container_name: ollama-service
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ../models:/models
    networks:
      - badge-network
    restart: unless-stopped
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "ollama serve &
       sleep 10 &&
       ollama create phi4-chat -f /models/Modelfile &&
       wait"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      # Set the CUDA_VISIBLE_DEVICES to the specific GPU UUID using nvidia-container-runtime 
      - CUDA_VISIBLE_DEVICES=GPU-63a7d2f4-b919-2de9-6a7c-25cb1b598936

  badge-api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: docker-badge-api
    container_name: badge-api
    ports:
      - "8000:8000"
    volumes:
      - ../assets:/app/assets
    environment:
      - OLLAMA_API_URL=http://ollama:11434/api/generate
    depends_on:
      - ollama
    networks:
      - badge-network
    restart: unless-stopped

networks:
  badge-network:
    driver: bridge

volumes:
  ollama-data: